{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mechanistic_model import ODE_model, prepare_ODE_model_data, fit_ODE_model\n",
    "from scipy.integrate import odeint\n",
    "from pygom import SimulateOde\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "# setting up the plotting\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "plt.rcParams['font.size'] = 12 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating ABC posteriors\n",
    "\n",
    "The model used to simulated epidemics is stored in mechanisitic_model.py alongside other utility functions. Although the model is relatively simple, care has been tkaen to ensure erlang distributed exposed and infected compartments, as well as accounting for detcted and undetected cases.\n",
    "\n",
    "All dates are inputted relative to the first day in the dataset. The 'assumed_lag' parameter dictates how many post-intervention data points are included in the fitting process. This allows for the incorporation of more data points, but care should betaken to ensure it remains below the incubaiton period of the disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcitons for weighting and parameterising the NegBinomLoss function\n",
    "\n",
    "def calculate_fit_weights(data):\n",
    "    weights = [1]\n",
    "    \n",
    "    for x in range(1, len(data)):\n",
    "        if data[x] == 0:\n",
    "            weight = 0.0001\n",
    "        else:\n",
    "            weight = (data[x] - data[x-1])/data[x]\n",
    "        \n",
    "        if weight == 0:\n",
    "            weight = 0.00001 # Note weights can not be zero\n",
    "        weights.append(weight)\n",
    "    return weights\n",
    "\n",
    "# Estimating alpha for NegBinomLoss \n",
    "# adapted from https://towardsdatascience.com/negative-binomial-regression-f99031bb25b4\n",
    "def fit_poisson(endog, exog, weights):\n",
    "    poiss = sm.GLM(endog, exog, family=sm.families.Poisson(), weights=weights).fit()\n",
    "    lambdas = poiss.mu\n",
    "    df_train = pd.DataFrame(columns=['BB_count', 'BB_lambda', 'Aux_OLS_dep'])\n",
    "    df_train['BB_count']=endog\n",
    "    df_train['BB_lambda']=lambdas\n",
    "    return df_train\n",
    "\n",
    "\n",
    "def fit_auxiliary_OLS(df_train):\n",
    "    df_train['Aux_OLS_dep'] = df_train.apply(lambda x:\\\n",
    "                                             ((x['BB_count']\\\n",
    "                                               - x['BB_lambda'])**2 \\\n",
    "                                               - x['BB_count'])\\\n",
    "                                               / x['BB_lambda'], axis=1)   \n",
    "    ols_expr = \"\"\"Aux_OLS_dep ~ BB_lambda - 1\"\"\"\n",
    "    aux_olsr_results = smf.ols(ols_expr, df_train).fit()\n",
    "    return abs(aux_olsr_results.params.BB_lambda)\n",
    "\n",
    "\n",
    "def estimate_over_dispersion(data, weights):\n",
    "    endog = data\n",
    "    exog = np.linspace(0, len(data)-1, len(data))\n",
    "    df_train = fit_poisson(endog, exog, weights)\n",
    "    alpha = fit_auxiliary_OLS(df_train)\n",
    "    k = 1/alpha\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_pop_dict = {'DK':5.8e6, 'NO':5.37e6, 'SE':1.02e7}\n",
    "int_day_dict = {'SE':10, 'DK':11, 'NO':6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify parameters\n",
    "country = 'DK' # Two letter country code\n",
    "metric = 'new_hosps' # 'new_cases' or 'new_hosps'\n",
    "N0 = nat_pop_dict[country]\n",
    "start_date = 0 # Pretty much always zero...\n",
    "int_date = int_day_dict[country]-start_date # Date of closures\n",
    "end_date = 28 # Final date of simulation\n",
    "assumed_lag = 5 # 5 days for cases and hospitalisations\n",
    "num_sims = 150 # Number of posterior parameterisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data and extract the useful parts\n",
    "nat_data = pd.read_csv('raw_data/{}_data.csv'.format(country))\n",
    "nat_data = nat_data.filter([metric]) \n",
    "nat_data = nat_data.dropna(axis=0)\n",
    "# Want to save this as a seperate dataframe\n",
    "df = pd.DataFrame(columns=[metric])#\n",
    "df[metric] = [int(x) for x in nat_data[metric]]\n",
    "df.to_csv('GP_input_data/{}_{}.csv'.format(country, metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABC fitting\n",
    "Functions initialising and running the ABC fitting process can be found in mechanisitic_model.py. Posteriors are estimated for all parameters, with the exception of $\\alpha_i$, which is set to 1.7, such that $\\sum_{i=0}^2\\alpha_i = 5.1$ - a reasonable approximation of the known incubation period of Covid-19. Initial Exposed populaiton is also estimated during the ABC fitting process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparations for ABC fitting\n",
    "fit_data, fit_end_point = prepare_ODE_model_data(df=nat_data, \n",
    "                                                 fit_to=metric, \n",
    "                                                 N_to_fit=int_date+assumed_lag)\n",
    "# Estimate over dispersion (k) of fitting data and calculate weightings\n",
    "weights = calculate_fit_weights(fit_data)\n",
    "k = estimate_over_dispersion(fit_data, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n",
      "BOX BOUNDS: [(0, 3), (0, 5), (0, 1), (0, 21), (0, 21), (0.25, 4), (0.25, 4), (0.25, 4), (0.25, 4), (0.25, 4), (0.25, 1)]\n",
      "Using NegBinom loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/InterventionsNNpy37/lib/python3.7/site-packages/pygom-0.1.8.dev15+g8c6d4ce.d20201111-py3.7-linux-x86_64.egg/pygom/utilR/distn.py:515: RuntimeWarning: divide by zero encountered in log\n",
      "  logpmf_p4= x*(np.log(mu) - np.log(k + mu))\n",
      "/home/ubuntu/anaconda3/envs/InterventionsNNpy37/lib/python3.7/site-packages/pygom-0.1.8.dev15+g8c6d4ce.d20201111-py3.7-linux-x86_64.egg/pygom/utilR/distn.py:515: RuntimeWarning: invalid value encountered in multiply\n",
      "  logpmf_p4= x*(np.log(mu) - np.log(k + mu))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 \n",
      " tolerance = inf \n",
      " acceptance rate = 100.00%\n",
      "\n",
      "Generation 2 \n",
      " tolerance = 125.28987 \n",
      " acceptance rate = 2.96%\n",
      "\n",
      "Generation 3 \n",
      " tolerance = 59.34215 \n",
      " acceptance rate = 3.31%\n",
      "\n",
      "Generation 4 \n",
      " tolerance = 43.91563 \n",
      " acceptance rate = 2.56%\n",
      "\n",
      "Generation 5 \n",
      " tolerance = 39.23275 \n",
      " acceptance rate = 2.18%\n",
      "\n",
      "Generation 6 \n",
      " tolerance = 37.30372 \n",
      " acceptance rate = 2.11%\n",
      "\n",
      "Generation 7 \n",
      " tolerance = 36.43993 \n",
      " acceptance rate = 1.61%\n",
      "\n",
      "Generation 8 \n",
      " tolerance = 35.93679 \n",
      " acceptance rate = 1.31%\n",
      "\n",
      "Generation 9 \n",
      " tolerance = 35.57646 \n",
      " acceptance rate = 1.06%\n",
      "\n",
      "Generation 10 \n",
      " tolerance = 35.32308 \n",
      " acceptance rate = 1.11%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run ABC fitting. 10 generations is usually sufficient. \n",
    "ode_model, fitted_model = fit_ODE_model(full_data=nat_data,\n",
    "                                         fit_data=fit_data, \n",
    "                                         fit_to=metric, \n",
    "                                         fit_end_point=fit_end_point,\n",
    "                                         weights=None,\n",
    "                                         k=k*np.ones(len(fit_data)),\n",
    "                                         N0=N0,\n",
    "                                         generations=10)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation \n",
    "\n",
    "A random selection of posteriors generated by the ABC fitting method are used to produce a simulated epidemic curve. Since the ABC fitting is done up to (or just past) intervention, it can be assumed that these can act as counter factuals to the observed cases under intervention. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ode_model\n",
    "times = np.linspace(0,end_date-1,end_date)\n",
    "\n",
    "curve_idx_dict = {'cases':7, 'hospitalisations':7, 'tot_hosps':5, 'new_cases':5}\n",
    "curve_idx = curve_idx_dict[metric]\n",
    "n = 0\n",
    "pred = np.zeros((num_sims,len(times)))\n",
    "sim = np.zeros((num_sims,len(times)))\n",
    "I0 = 0\n",
    "Eb0 = 0\n",
    "Ec0 = 0\n",
    "Id0 = 0\n",
    "Iu0 = 0\n",
    "R0  = 0\n",
    "Id_cum0 = 0\n",
    "null_series = 0\n",
    "\n",
    "for i in range(num_sims):\n",
    "    print('Simulating posterioir {}'.format(i))\n",
    "    p_beta, p_gamma, p_kappa, p_delta1, p_delta2, p_Ea0, p_Eb0, p_Ec0, p_I0, p_Id0, p_Iu0 = fitted_model.res[i]\n",
    "    model.parameters = {'beta':p_beta,\n",
    "                        'alpha0':1.6,\n",
    "                        'alpha1':1.6,\n",
    "                        'alpha2':1.6,\n",
    "                        'gamma':p_gamma, \n",
    "                        'kappa':p_kappa,\n",
    "                        'delta1':p_delta1,\n",
    "                        'delta2':p_delta2,\n",
    "                        'N0':N0}\n",
    "\n",
    "    solution = odeint(model.ode,[N0-(p_Ea0+p_Eb0+p_Ec0+p_I0+p_Id0+p_Iu0+R0), \n",
    "                                 np.round(p_Ea0),np.round(p_Eb0),np.round(p_Ec0), np.round(p_I0), np.round(p_Id0)\n",
    "                                 , np.round(p_Iu0), Id_cum0, R0],times).T[curve_idx]\n",
    "    if sum(solution) != 0:\n",
    "        pred[i] = solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the simulated data\n",
    "simulated_data = {}\n",
    "for idx, data in enumerate(pred):\n",
    "    simulated_data.update({idx:data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "det_df = pd.DataFrame.from_dict(simulated_data)\n",
    "# Generally only interested in stochastic data, although deterministic can be useful for longer sims.\n",
    "det_df.to_csv('ABC_posteriors/posteriors_{}_{}.csv'.format(metric, country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to check everything looks sensible.\n",
    "#start_date = int_date-10\n",
    "df = pd.read_csv('ABC_posteriors/posteriors_{}_{}.csv'.format(metric, country))\n",
    "for x in range(100):\n",
    "    plt.plot(pred[x][start_date:], 'r', alpha=0.2)\n",
    "plt.plot(times, nat_data[metric][:end_date], 'x', label = 'Observed data')\n",
    "plt.plot(np.median(pred, axis=0)[start_date:end_date], 'k--', label = 'Median forecast')\n",
    "plt.title('ABC Posteriors for {}'.format(country))\n",
    "plt.ylabel('Cumulative cases')\n",
    "plt.xlabel('Days')\n",
    "plt.axvline(x=int_date, linewidth=4, color=\"r\")\n",
    "#plt.axvline(x=int_date+assumed_lag, linewidth=2, color=\"r\")\n",
    "plt.legend()\n",
    "#plt.ylim([0, 10000])\n",
    "#plt.xlim([0,30])\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check that num observations and num simulated data points are the same!\n",
    "assert len(pred[0])==len(nat_data[metric][start_date:end_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some cleaning up of the dataframe to get rid of unnecessary bits\n",
    "fit_data = [int(x) for x in nat_data[metric]]\n",
    "fit_dict = {metric:fit_data}\n",
    "fit_df = pd.DataFrame.from_dict(fit_dict)\n",
    "fit_df.to_csv('GP_input_data/{}_{}.csv'.format(country, metric, assumed_lag))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
